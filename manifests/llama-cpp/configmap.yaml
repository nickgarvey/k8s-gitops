apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-config
  namespace: llama-cpp
  labels:
    app.kubernetes.io/managed-by: ngarvey-homelab
data:
  # Model configurations in JSON format
  # Each model has: repo (HuggingFace repo), filter (quantization), mmproj (vision model projector, optional)
  MODELS_CONFIG: |
    [
      {
        "name": "qwen3-80b",
        "repo": "unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF",
        "filter": "Q5_K_M",
        "mmproj": ""
      },
      {
        "name": "qwen3-vl-30b",
        "repo": "unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF",
        "filter": "Q8_K_XL",
        "mmproj": "mmproj-F16.gguf"
      },
      {
        "name": "qwen3-vl-235b",
        "repo": "unsloth/Qwen3-VL-235B-A22B-Instruct-1M-GGUF",
        "filter": "Q3_K_S",
        "mmproj": "mmproj-F16.gguf"
      }
    ]

  # llama-server arguments for router mode (one per line, will be passed as command-line args)
  # Note: --model is NOT specified here; router mode uses --models-dir instead
  LLAMA_ARGS: |
    --host 0.0.0.0
    --port 8080
    --n-gpu-layers 99
    --ctx-size 65536
    --metrics
    --batch-size 512
    --ubatch-size 512
    --jinja
    --models-max 2
    --timeout 1800
