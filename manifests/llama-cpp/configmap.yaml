apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-cpp-config
  namespace: llama-cpp
  labels:
    app.kubernetes.io/managed-by: ngarvey-homelab
data:
  # HuggingFace model repo (e.g., "ggml-org/gpt-oss-120b-GGUF" or "bartowski/Llama-3.2-1B-Instruct-GGUF")
  MODEL_REPO: "unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF"

  # Optional: filter pattern to select specific quantization (e.g., "Q4_K_M", "mxfp4", "Q8_0")
  # Leave empty to download all GGUF files in the repo
  MODEL_FILTER: "Q5_K_M"

  # llama-server arguments (one per line, will be passed as command-line args)
  LLAMA_ARGS: |
    --host 0.0.0.0
    --port 8080
    --n-gpu-layers 99
    --ctx-size 65536
    --metrics
    --batch-size 4096
    --ubatch-size 512
    --jinja
