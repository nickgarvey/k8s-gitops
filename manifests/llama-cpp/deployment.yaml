apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-cpp
  namespace: llama-cpp
  labels:
    app.kubernetes.io/managed-by: ngarvey-homelab
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-cpp
  template:
    metadata:
      labels:
        app: llama-cpp
        app.kubernetes.io/managed-by: ngarvey-homelab
    spec:
      nodeSelector:
        kubernetes.io/hostname: framework
      initContainers:
        - name: download-models
          image: python:3.12-alpine
          command:
            - python3
            - /scripts/download-models.py
          env:
            - name: MODELS_CONFIG
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-config
                  key: MODELS_CONFIG
          volumeMounts:
            - name: models
              mountPath: /models
            - name: scripts
              mountPath: /scripts
      containers:
        - name: llama-server
          image: zot.home.arpa:5000/llama-cpp-vulkan-ngarvey:latest
          command:
            - "/bin/sh"
            - "-c"
            - |
              echo "Starting llama-server in router mode"
              echo "Available models:"
              ls -la /models/

              # Build args from LLAMA_ARGS configmap
              # In router mode, we use --models-dir instead of --model
              ARGS="--models-dir /models"
              if [ -n "$LLAMA_ARGS" ]; then
                # Convert newline-separated args to space-separated
                ARGS="$ARGS $(echo "$LLAMA_ARGS" | tr '\n' ' ')"
              fi

              echo "Starting llama-server with args: $ARGS"
              exec /bin/llama-server $ARGS
          env:
            - name: VK_ICD_FILENAMES
              value: "/share/vulkan/icd.d/radeon_icd.x86_64.json"
            - name: LLAMA_ARGS
              valueFrom:
                configMapKeyRef:
                  name: llama-cpp-config
                  key: LLAMA_ARGS
          ports:
            - containerPort: 8080
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 120
            periodSeconds: 30
            failureThreshold: 3
          volumeMounts:
            - name: models
              mountPath: /models
            - name: dri
              mountPath: /dev/dri
          securityContext:
            privileged: true
      volumes:
        - name: models
          hostPath:
            path: /var/lib/rancher/k3s/storage/llama-cpp-models
            type: DirectoryOrCreate
        - name: dri
          hostPath:
            path: /dev/dri
            type: Directory
        - name: scripts
          configMap:
            name: llama-cpp-scripts
            defaultMode: 0755
